{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1wWFrCLgSUF"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-huggingface langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "import getpass\n",
        "import os\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "dldRZSQUgpg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = getpass.getpass(\"Enter your HuggingFace Token key: \")"
      ],
      "metadata": {
        "id": "Gj7m5osfimIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(API_KEY)"
      ],
      "metadata": {
        "id": "q5-0JiPXhhAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ],
      "metadata": {
        "id": "t7kdleljgwHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatHuggingFace(llm=endpoint, verbose=True)"
      ],
      "metadata": {
        "id": "zONmTDA6g5W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "id": "rwTpjrA_hL1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import TypedDict, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage"
      ],
      "metadata": {
        "id": "aX-K93QbhTY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "\n",
        "    text: str  # Stores the original input text\n",
        "    classification: str  # Represents the classification result (e.g., category label)\n",
        "    entities: List[str]  # Holds a list of extracted entities (e.g., named entities)\n",
        "    summary: str  # Stores a summarized version of the text"
      ],
      "metadata": {
        "id": "I-Qa3p5Xh6IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_node(state: State):\n",
        "   \"\"\"\n",
        "   Classify the text into one of predefined categories.\n",
        "\n",
        "   Parameters:\n",
        "       state (State): The current state dictionary containing the text to classify\n",
        "\n",
        "   Returns:\n",
        "       dict: A dictionary with the \"classification\" key containing the category result\n",
        "\n",
        "   Categories:\n",
        "       - News: Factual reporting of current events\n",
        "       - Blog: Personal or informal web writing\n",
        "       - Research: Academic or scientific content\n",
        "       - Other: Content that doesn't fit the above categories\n",
        "   \"\"\"\n",
        "\n",
        "   # Define a prompt template that asks the model to classify the given text\n",
        "   prompt = PromptTemplate(\n",
        "       input_variables=[\"text\"],\n",
        "       template=\"Classify the following text into one of the categories: News, Blog, Research, or Other.\\n\\nText: {text}\\n\\nCategory:\"\n",
        "   )\n",
        "\n",
        "   # Format the prompt with the input text from the state\n",
        "   message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
        "\n",
        "   # Invoke the language model to classify the text based on the prompt\n",
        "   classification = llm.invoke([message]).content.strip()\n",
        "\n",
        "   # Return the classification result in a dictionary\n",
        "   return {\"classification\": classification}\n",
        "\n",
        "\n",
        "def entity_extraction_node(state: State):\n",
        "  # Function to identify and extract named entities from text\n",
        "  # Organized by category (Person, Organization, Location)\n",
        "\n",
        "  # Create template for entity extraction prompt\n",
        "  # Specifies what entities to look for and format (comma-separated)\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"text\"],\n",
        "      template=\"Extract all the entities (Person, Organization, Location) from the following text. Provide the result as a comma-separated list.\\n\\nText: {text}\\n\\nEntities:\"\n",
        "  )\n",
        "\n",
        "  # Format the prompt with text from state and wrap in HumanMessage\n",
        "  message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
        "\n",
        "  # Send to language model, get response, clean whitespace, split into list\n",
        "  entities = llm.invoke([message]).content.strip().split(\", \")\n",
        "\n",
        "  # Return dictionary with entities list to be merged into agent state\n",
        "  return {\"entities\": entities}\n",
        "\n",
        "\n",
        "def summarize_node(state: State):\n",
        "    # Create a template for the summarization prompt\n",
        "    # This tells the model to summarize the input text in one sentence\n",
        "    summarization_prompt = PromptTemplate.from_template(\n",
        "        \"\"\"Summarize the following text in one short sentence.\n",
        "\n",
        "        Text: {text}\n",
        "\n",
        "        Summary:\"\"\"\n",
        "    )\n",
        "\n",
        "    # Create a chain by connecting the prompt template to the language model\n",
        "    # The \"|\" operator pipes the output of the prompt into the model\n",
        "    chain = summarization_prompt | llm\n",
        "\n",
        "    # Execute the chain with the input text from the state dictionary\n",
        "    # This passes the text to be summarized to the model\n",
        "    response = chain.invoke({\"text\": state[\"text\"]})\n",
        "\n",
        "    # Return a dictionary with the summary extracted from the model's response\n",
        "    # This will be merged into the agent's state\n",
        "    return {\"summary\": response.content}\n",
        ""
      ],
      "metadata": {
        "id": "1JKRBquLh8ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(State)\n",
        "\n",
        "# Add nodes to the graph\n",
        "workflow.add_node(\"classification_node\", classification_node)\n",
        "workflow.add_node(\"entity_extraction\", entity_extraction_node)\n",
        "workflow.add_node(\"summarization\", summarize_node)\n",
        "\n",
        "# Add edges to the graph\n",
        "workflow.set_entry_point(\"classification_node\") # Set the entry point of the graph\n",
        "workflow.add_edge(\"classification_node\", \"entity_extraction\")\n",
        "workflow.add_edge(\"entity_extraction\", \"summarization\")\n",
        "workflow.add_edge(\"summarization\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "ni63kUNziCVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample text about Anthropic's MCP to test our agent\n",
        "sample_text = \"\"\"\n",
        "Anthropic's MCP (Model Context Protocol) is an open-source powerhouse that lets your applications interact effortlessly with APIs across various systems.\n",
        "\"\"\"\n",
        "\n",
        "# Create the initial state with our sample text\n",
        "state_input = {\"text\": sample_text}\n",
        "\n",
        "# Run the agent's full workflow on our sample text\n",
        "result = app.invoke(state_input)"
      ],
      "metadata": {
        "id": "wuAsz3tsiEM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print each component of the result:\n",
        "# - The classification category (News, Blog, Research, or Other) ### -> Expected Response: Technology\n",
        "print(\"Classification:\", result[\"classification\"])\n",
        "\n",
        "# - The extracted entities (People, Organizations, Locations) ### -> Expected Response [‘Anthropic’, ‘MCP’, ‘Model Context Protocol’]\n",
        "print(\"\\nEntities:\", result[\"entities\"])\n",
        "\n",
        "# - The generated summary of the text\n",
        "print(\"\\nSummary:\", result[\"summary\"])"
      ],
      "metadata": {
        "id": "p3--fg7tiGTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}